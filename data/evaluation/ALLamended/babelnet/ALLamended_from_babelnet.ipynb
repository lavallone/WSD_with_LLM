{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the mapping synset --> babelnet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212405\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "synset2babel = {}\n",
    "with open(\"babelnet/sk2bn_id.tsv\", \"r\") as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        key, value = row\n",
    "        synset2babel[value] = key\n",
    "\n",
    "print(len(synset2babel.keys()))\n",
    "with open(\"babelnet/synset2babel.json\", \"w\") as file:\n",
    "    json.dump(synset2babel, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step I: format the original ALLamended dataset in our format and only add the infos we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "gold_key_path = \"xml/ALLamended.gold.key.txt\"\n",
    "xml_data_path = \"xml/ALLamended.data.xml\"\n",
    "output_json_path = \"ALLamended_preprocessed.json\"\n",
    "\n",
    "# Load the gold key file\n",
    "gold_key_mapping = {}\n",
    "with open(gold_key_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            instance_id, senses = parts[0], parts[1:]\n",
    "            gold_key_mapping[instance_id] = senses\n",
    "\n",
    "# Parse the XML data\n",
    "tree = ET.parse(xml_data_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "preprocessed_data = []\n",
    "for text in root.findall(\".//text\"):\n",
    "    for sentence in text.findall(\".//sentence\"):\n",
    "        tokens = []\n",
    "        for elem in sentence:\n",
    "            if elem.tag == \"wf\" or elem.tag == \"instance\":\n",
    "                tokens.append(elem.text)\n",
    "        context = \" \".join(tokens)\n",
    "        \n",
    "        for instance in sentence.findall(\"instance\"):\n",
    "            instance_id = instance.get(\"id\")\n",
    "            word = instance.text\n",
    "            lemma = instance.get(\"lemma\")\n",
    "            pos = instance.get(\"pos\")\n",
    "            senses = [ synset2babel[elem] for elem in gold_key_mapping.get(instance_id, []) ] # we already apply the babelnet mapping\n",
    "            \n",
    "            entry = {\n",
    "                \"id\": instance_id,\n",
    "                \"text\": context,\n",
    "                \"word\": word,\n",
    "                \"lemma\": lemma,\n",
    "                \"pos\": pos,\n",
    "                \"gold\": senses,\n",
    "                \"gold_definitions\" : [],\n",
    "                \"candidates\": [],\n",
    "                \"definitions\": []\n",
    "            }\n",
    "            preprocessed_data.append(entry)\n",
    "\n",
    "# Save the data to JSON\n",
    "with open(\"ALLamended_preprocessed.json\", 'w') as outfile:\n",
    "    json.dump(preprocessed_data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step II: fill \"candidates\" and \"definitions\" fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved to: ALLamended_preprocessed.json\n"
     ]
    }
   ],
   "source": [
    "# Read the JSONL file and build a lookup dictionary for lemma and pos\n",
    "with open(\"ALLamended_preprocessed.json\", \"r\") as file:\n",
    "    all_amended_data = json.load(file)\n",
    "\n",
    "output_entries = []\n",
    "with open(\"babelnet/output.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line)\n",
    "        output_entries.append(entry)\n",
    "\n",
    "lookup = {}\n",
    "for entry in output_entries:\n",
    "    key = (entry.get(\"lemma\"), entry.get(\"pos\"))\n",
    "    lookup[key] = [] # we need to initialize each time because in output.jsonl there are duplicates!!!!!\n",
    "    for elem in entry.get(\"synsets\"):\n",
    "        lookup[key].append({\n",
    "            \"id\": elem.get(\"id\"),\n",
    "            \"main_gloss\": elem.get(\"main_gloss\")\n",
    "        })\n",
    "\n",
    "# Update the \"candidates\" and \"definitions\" fields in ALLamended_preprocessed.json\n",
    "for item in all_amended_data:\n",
    "    key = (item.get(\"lemma\"), item.get(\"pos\"))\n",
    "    matches = lookup.get(key, [])\n",
    "    assert  matches is not [] # it should never happen\n",
    "    item[\"candidates\"] = [match[\"id\"] for match in matches]\n",
    "    item[\"definitions\"] = [match[\"main_gloss\"] for match in matches]\n",
    "\n",
    "# Save the updated data to a new JSON file\n",
    "updated_file_path = \"ALLamended_preprocessed.json\"\n",
    "with open(updated_file_path, \"w\") as file:\n",
    "    json.dump(all_amended_data, file, indent=4)\n",
    "print(f\"Updated file saved to: {updated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step III: add the gold definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved to: ALLamended_preprocessed.json\n"
     ]
    }
   ],
   "source": [
    "with open(\"ALLamended_preprocessed.json\", \"r\") as file:\n",
    "    all_amended_data = json.load(file)\n",
    "\n",
    "# fill gold_definitions\n",
    "for item in all_amended_data:\n",
    "    gold_definitions = []\n",
    "    for gold in item[\"gold\"]:\n",
    "        for i,candidate in enumerate(item[\"candidates\"]):\n",
    "            if gold == candidate: gold_definitions.append(item[\"definitions\"][i])\n",
    "    item[\"gold_definitions\"] = gold_definitions\n",
    "\n",
    "# check if the gold is in the candidates\n",
    "for item in all_amended_data:\n",
    "    for gold in item[\"gold\"]:\n",
    "        assert gold in item[\"candidates\"]\n",
    "\n",
    "# check if all the candidates have different names\n",
    "for item in all_amended_data:\n",
    "    assert len(item[\"candidates\"]) == len(set(item[\"candidates\"]))\n",
    "\n",
    "updated_file_path = \"ALLamended_preprocessed.json\"\n",
    "with open(updated_file_path, \"w\") as file:\n",
    "    json.dump(all_amended_data, file, indent=4)\n",
    "print(f\"Updated file saved to: {updated_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
